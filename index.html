<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Gemini Visual Assistant</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        /* Custom style for the video feed to ensure it's mirrored (like a selfie camera) */
        #videoElement {
            transform: scaleX(-1);
        }
    </style>
</head>
<body class="bg-gray-900 text-white min-h-screen flex flex-col items-center justify-center p-4 font-sans">

    <div class="w-full max-w-2xl bg-gray-800 rounded-2xl shadow-2xl p-6 md:p-8 space-y-6">
        <h1 class="text-3xl font-bold text-center text-cyan-400">Gemini Visual Assistant</h1>

        <div class="relative w-full bg-gray-900 rounded-lg overflow-hidden shadow-lg aspect-video">
            <video id="videoElement" class="w-full h-full object-cover" autoplay muted playsinline></video>
            <canvas id="captureCanvas" class="hidden"></canvas>
        </div>

        <div class="flex justify-center space-x-4">
            <button id="startButton" class="bg-cyan-500 hover:bg-cyan-600 text-white font-bold py-3 px-8 rounded-full text-lg transition-all duration-300 shadow-lg focus:outline-none focus:ring-4 focus:ring-cyan-300 disabled:bg-gray-600 disabled:cursor-not-allowed">
                Start Assistant
            </button>
            
            <button id="stopButton" class="bg-red-500 hover:bg-red-600 text-white font-bold py-3 px-8 rounded-full text-lg transition-all duration-300 shadow-lg focus:outline-none focus:ring-4 focus:ring-red-300 disabled:bg-gray-600 disabled:cursor-not-allowed" disabled>
                Stop Assistant
            </button>
        </div>

        <div class="space-y-4 text-center">
            <div id="status" class="text-lg text-gray-400 h-6">
                Click "Start" to begin...
            </div>
            
            <div class="bg-gray-700 p-4 rounded-lg min-h-[100px] flex items-center justify-center shadow-inner">
                <p id="responseText" class="text-xl font-medium"></p>
            </div>
        </div>
    </div>

    <script>
        // --- 1. DOM Elements ---
        const videoElement = document.getElementById('videoElement');
        const canvas = document.getElementById('captureCanvas');
        const startButton = document.getElementById('startButton');
        const stopButton = document.getElementById('stopButton'); // <-- ADDED
        const statusElement = document.getElementById('status');
        const responseTextElement = document.getElementById('responseText');

        // --- ADDED: State variables ---
        let isAssistantActive = false;
        let stream = null; // To keep track of the camera stream

        // --- 2. API Configuration ---
        // NOTE: Leave apiKey as "" - it will be automatically handled by the environment.
        const apiKey = "AIzaSyAAmR_3eXcZwkXMFyhyErk1RSkrvJgYU_A"; // <-- REMEMBER TO PUT YOUR API KEY HERE
        const apiUrl = `https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-09-2025:generateContent?key=${apiKey}`;

        // --- 3. Web Speech API Setup ---
        
        // --- Speech Recognition (Speech-to-Text) ---
        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
        let recognition;

        if (SpeechRecognition) {
            recognition = new SpeechRecognition();
            recognition.continuous = false;
            recognition.interimResults = false;
            recognition.lang = 'en-US';

            recognition.onresult = (event) => {
                const transcript = event.results[0][0].transcript;
                statusElement.textContent = `Heard: "${transcript}". Processing...`;
                responseTextElement.textContent = ""; 
                
                const base64ImageData = captureFrame();
                if (base64ImageData) {
                    getGeminiResponse(transcript, base64ImageData);
                } else {
                    speak("I couldn't capture the video frame. Please try again.");
                }
            };

            recognition.onerror = (event) => {
                console.error("Speech recognition error:", event.error);
                if (isAssistantActive) { // Only show error if we are supposed to be active
                    statusElement.textContent = "Sorry, I didn't catch that. Listening again...";
                }
            };

            recognition.onend = () => {
                // This is now handled by the speak() function's onend
                // We only re-listen if the assistant is active
                if (isAssistantActive) {
                    // This is a failsafe in case speak() doesn't trigger
                    // But the main loop is in speak.onend
                }
            };

        } else {
            statusElement.textContent = "Sorry, your browser doesn't support speech recognition.";
            startButton.disabled = true;
        }

        // --- Speech Synthesis (Text-to-Speech) ---
        const synth = window.speechSynthesis;

        function speak(text) {
            if (synth.speaking) {
                console.error("Speech synthesis is already speaking.");
                return;
            }

            const utterance = new SpeechSynthesisUtterance(text);
            
            utterance.onend = () => {
                // MODIFIED: Only start listening again if the assistant is active
                if (isAssistantActive && SpeechRecognition) {
                    startListening();
                }
            };

            utterance.onerror = (event) => {
                console.error("Speech synthesis error:", event.error);
                 // Even on error, try to restart listening loop
                if (isAssistantActive && SpeechRecognition) {
                    startListening();
                }
            };
            
            synth.speak(utterance);
        }

        // --- 4. Main Application Logic ---

        async function startApp() {
            statusElement.textContent = "Requesting permissions...";
            try {
                // MODIFIED: Store stream in global variable
                stream = await navigator.mediaDevices.getUserMedia({ 
                    video: { facingMode: 'environment' }, 
                    audio: true 
                });

                videoElement.srcObject = stream;
                await videoElement.play(); 
                
                // MODIFIED: Update UI
                startButton.textContent = "Assistant is Active";
                startButton.disabled = true;
                stopButton.disabled = false; // <-- ADDED
                isAssistantActive = true; // <-- ADDED
                
                speak("Hi! I'm ready. What are you looking for?");

            } catch (err) {
                console.error("Error accessing media devices.", err);
                statusElement.textContent = "Error: Could not access camera or microphone.";
            }
        }

        function startListening() {
            // MODIFIED: Check if active and use try/catch
            if (SpeechRecognition && isAssistantActive) { 
                statusElement.textContent = "Listening...";
                try {
                    recognition.start();
                } catch (e) {
                    console.error("Recognition service error:", e);
                    statusElement.textContent = "Trying to listen again...";
                }
            }
        }

        // --- ADDED: New function to stop the assistant ---
        function stopApp() {
            isAssistantActive = false; // This stops the loops

            // Stop speech recognition and synthesis
            if (recognition) {
                recognition.stop();
            }
            if (synth) {
                synth.cancel();
            }

            // Stop the camera and microphone stream
            if (stream) {
                stream.getTracks().forEach(track => track.stop());
                videoElement.srcObject = null;
            }

            // Reset UI
            statusElement.textContent = "Click 'Start' to begin...";
            responseTextElement.textContent = "";
            startButton.textContent = "Start Assistant";
            startButton.disabled = false;
            stopButton.disabled = true;
        }

        function captureFrame() {
            try {
                const context = canvas.getContext('2d');
                canvas.width = videoElement.videoWidth;
                canvas.height = videoElement.videoHeight;

                context.save();
                context.scale(-1, 1); 
                context.drawImage(videoElement, -canvas.width, 0, canvas.width, canvas.height);
                context.restore();

                const dataUrl = canvas.toDataURL('image/jpeg', 0.7); 
                return dataUrl.split(',')[1];
            } catch (err) {
                console.error("Error capturing frame:", err);
                return null;
            }
        }

        // --- 5. Gemini API Call ---

        async function getGeminiResponse(text, base64ImageData) {
            // ADDED: Check if assistant is active
            if (!isAssistantActive) {
                console.log("Assistant was stopped. Cancelling API request.");
                return;
            }

            statusElement.textContent = "Thinking...";

            const systemPrompt = "You are a helpful visual assistant. Your user is showing you a video feed and asking you questions. Answer their question based on what you see in the image. Be concise and descriptive. For example, if they ask 'where is my bottle?', say 'I see a bottle on the right side of the table.'";

            const payload = {
                systemInstruction: {
                    parts: [{ text: systemPrompt }]
                },
                contents: [
                    {
                        role: "user",
                        parts: [
                            { text: text },
                            {
                                inlineData: {
                                    mimeType: "image/jpeg",
                                    data: base64ImageData
                                }
                            }
                        ]
                    }
                ]
            };

            try {
                const response = await fetchWithBackoff(apiUrl, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify(payload)
                });

                if (!response.ok) {
                    throw new Error(`HTTP error! status: ${response.status}`);
                }

                const result = await response.json();
                
                const candidate = result.candidates?.[0];
                if (candidate && candidate.content?.parts?.[0]?.text) {
                    const responseText = candidate.content.parts[0].text;
                    responseTextElement.textContent = responseText;
                    speak(responseText);
                } else {
                    console.error("Unexpected API response structure:", result);
                    speak("Sorry, I received an unusual response from the server.");
                }

            } catch (error) {
                console.error('Error calling Gemini API:', error);
                statusElement.textContent = "Error connecting to AI. Please try again.";
                speak("I'm having trouble connecting. Let's try again in a moment.");
            }
        }

        async function fetchWithBackoff(url, options, retries = 3, delay = 1000) {
            try {
                return await fetch(url, options);
            } catch (err) {
                if (retries > 0) {
                    await new Promise(resolve => setTimeout(resolve, delay));
                    return fetchWithBackoff(url, options, retries - 1, delay * 2); 
                } else {
                    throw err;
                }
            }
        }
        
        // --- 6. Event Listeners ---
        startButton.addEventListener('click', startApp);
        stopButton.addEventListener('click', stopApp); // <-- ADDED
    </script>
</body>
</html>
